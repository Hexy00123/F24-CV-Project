{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchinfo\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "from vision_transformer.VisionTransformer import ViT\n",
    "from src.read_config import read_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc/Documents/Innopolis/F24/CV/Project/notebooks/../src/read_config.py:8: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  hydra.initialize(config_path=config_path)\n"
     ]
    }
   ],
   "source": [
    "config = read_config(config_path='../configs', config_name='transformer_params.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module): \n",
    "    def __init__(self, n_classes, img_size: int, in_channels: int, transformer_config): \n",
    "        super().__init__()\n",
    "        \n",
    "        self.vit = ViT(img_size=img_size, in_channels=in_channels, **transformer_config)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.vit.d_model, self.vit.d_model//2), \n",
    "            nn.ReLU(), \n",
    "            \n",
    "            nn.Linear(self.vit.d_model // 2, n_classes), \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x): \n",
    "        embeddins = self.vit(x)\n",
    "        logits = self.classifier(embeddins)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "ViT                                                [15, 768]                 768\n",
       "├─PatchEmbedder: 1-1                               [15, 16, 768]             --\n",
       "│    └─Conv2d: 2-1                                 [15, 768, 4, 4]           148,224\n",
       "├─PositionalEncoding: 1-2                          [15, 16, 768]             --\n",
       "├─Encoder: 1-3                                     [15, 17, 768]             --\n",
       "│    └─ModuleList: 2-2                             --                        --\n",
       "│    │    └─EncoderBlock: 3-1                      [15, 17, 768]             5,510,916\n",
       "│    │    └─EncoderBlock: 3-2                      [15, 17, 768]             5,510,916\n",
       "│    │    └─EncoderBlock: 3-3                      [15, 17, 768]             5,510,916\n",
       "│    │    └─EncoderBlock: 3-4                      [15, 17, 768]             5,510,916\n",
       "│    │    └─EncoderBlock: 3-5                      [15, 17, 768]             5,510,916\n",
       "│    │    └─EncoderBlock: 3-6                      [15, 17, 768]             5,510,916\n",
       "│    │    └─EncoderBlock: 3-7                      [15, 17, 768]             5,510,916\n",
       "│    │    └─EncoderBlock: 3-8                      [15, 17, 768]             5,510,916\n",
       "│    └─LayerNorm: 2-3                              [15, 17, 768]             2\n",
       "====================================================================================================\n",
       "Total params: 44,236,322\n",
       "Trainable params: 44,236,322\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 696.88\n",
       "====================================================================================================\n",
       "Input size (MB): 0.18\n",
       "Forward/backward pass size (MB): 124.20\n",
       "Params size (MB): 176.94\n",
       "Estimated Total Size (MB): 301.33\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = TransformerClassifier(n_classes=10, img_size=32, in_channels=3, transformer_config=config['params'])\n",
    "\n",
    "torchinfo.summary(classifier.vit, (15, 3, 32, 32), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(torch.rand(15, 3, 32, 32)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
